# Multi-GPU Model Example: Llama 2 70B with 4 GPUs
# This example demonstrates layer-based sharding across 4 GPUs for a 70B parameter model
# Requires 4x L4 or A100 GPUs with 24GB+ VRAM each
apiVersion: inference.llmkube.dev/v1alpha1
kind: Model
metadata:
  name: llama-70b-multi-gpu
  namespace: default
spec:
  # Llama 2 70B Q4_K_M (~38GB model)
  source: https://huggingface.co/TheBloke/Llama-2-70B-GGUF/resolve/main/llama-2-70b.Q4_K_M.gguf
  format: gguf
  quantization: Q4_K_M

  hardware:
    accelerator: cuda
    gpu:
      enabled: true
      count: 4              # Use 4 GPUs for multi-GPU inference
      vendor: nvidia
      layers: -1            # -1 = offload all layers (auto-detect)

      # Optional: Configure sharding strategy
      sharding:
        strategy: layer     # Layer-based sharding (default)
        # For 70B models with 80 layers, distribute evenly:
        # GPU0: layers 0-19, GPU1: layers 20-39, GPU2: layers 40-59, GPU3: layers 60-79

  # Resource requirements for model processing
  resources:
    cpu: "8"
    memory: "64Gi"
---
apiVersion: inference.llmkube.dev/v1alpha1
kind: InferenceService
metadata:
  name: llama-70b-multi-gpu-service
  namespace: default
spec:
  modelRef: llama-70b-multi-gpu
  replicas: 1

  # Use CUDA-enabled llama.cpp image
  image: ghcr.io/ggerganov/llama.cpp:server-cuda

  resources:
    gpu: 4              # Request 4 GPUs per pod
    gpuMemory: "96Gi"   # 24Gi per GPU minimum (96Gi total)
    cpu: "8"
    memory: "32Gi"

  endpoint:
    port: 8080
    type: ClusterIP
