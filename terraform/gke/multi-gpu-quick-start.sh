#!/bin/bash
# Multi-GPU Quick Start Script for GKE
# For testing Issue #2 - Multi-GPU single-node support

set -e

echo "ğŸš€ LLMKube Multi-GPU GKE Cluster Setup"
echo "======================================"
echo "This will create a cluster with 2 GPUs per node for multi-GPU testing"
echo ""

# Check if gcloud is installed
if ! command -v gcloud &> /dev/null; then
    echo "âŒ gcloud CLI not found. Installing..."
    if [[ "$OSTYPE" == "darwin"* ]]; then
        brew install --cask google-cloud-sdk
    else
        echo "Please install gcloud CLI: https://cloud.google.com/sdk/docs/install"
        exit 1
    fi
fi

# Check if terraform is installed
if ! command -v terraform &> /dev/null; then
    echo "âŒ Terraform not found. Installing..."
    if [[ "$OSTYPE" == "darwin"* ]]; then
        brew install terraform
    else
        echo "Please install Terraform: https://www.terraform.io/downloads"
        exit 1
    fi
fi

# Install GKE auth plugin if not present
if ! command -v gke-gcloud-auth-plugin &> /dev/null; then
    echo "ğŸ“¦ Installing GKE auth plugin..."
    gcloud components install gke-gcloud-auth-plugin --quiet
fi

# Check if authenticated
if ! gcloud auth list --filter=status:ACTIVE --format="value(account)" | grep -q "."; then
    echo "ğŸ” Authenticating with GCP..."
    gcloud auth login
    gcloud auth application-default login
fi

# Get or set project
CURRENT_PROJECT=$(gcloud config get-value project 2>/dev/null)
if [ -z "$CURRENT_PROJECT" ]; then
    echo ""
    echo "ğŸ“‹ Available projects:"
    gcloud projects list
    echo ""
    read -p "Enter project ID: " PROJECT_ID
    gcloud config set project "$PROJECT_ID"
else
    PROJECT_ID=$CURRENT_PROJECT
    echo "âœ… Using project: $PROJECT_ID"
fi

# Enable required APIs
echo ""
echo "ğŸ”Œ Enabling required GCP APIs..."
gcloud services enable container.googleapis.com --project="$PROJECT_ID" 2>/dev/null || true
gcloud services enable compute.googleapis.com --project="$PROJECT_ID" 2>/dev/null || true

# Ask user to select GPU type
echo ""
echo "ğŸ® Select GPU configuration:"
echo "  1) 2x T4 GPUs (Recommended - Cost-effective ~$0.70/hr per node)"
echo "  2) 2x L4 GPUs (Better performance ~$1.40/hr per node)"
read -p "Choice (1 or 2): " GPU_CHOICE

if [ "$GPU_CHOICE" == "2" ]; then
    GPU_TYPE="nvidia-l4"
    MACHINE_TYPE="g2-standard-24"
    GPU_NAME="L4"
else
    GPU_TYPE="nvidia-tesla-t4"
    MACHINE_TYPE="n1-standard-8"
    GPU_NAME="T4"
fi

echo "âœ… Selected: 2x $GPU_NAME GPUs on $MACHINE_TYPE"

# Create terraform.tfvars
echo ""
echo "ğŸ“ Creating terraform.tfvars for multi-GPU configuration..."
cat > terraform.tfvars <<EOF
# Multi-GPU Configuration for LLMKube Testing (Issue #2)
# Auto-generated by multi-gpu-quick-start.sh

project_id   = "$PROJECT_ID"
region       = "us-west1"  # Better GPU availability
cluster_name = "llmkube-multi-gpu-test"

# Multi-GPU Configuration: 2 GPUs per node
gpu_type     = "$GPU_TYPE"
gpu_count    = 2
machine_type = "$MACHINE_TYPE"

# Auto-scaling (start at 0 to save money)
min_gpu_nodes = 0
max_gpu_nodes = 2

# Cost savings (~70% discount)
use_spot = true

# Storage (larger for 13B+ models)
disk_size_gb = 200
EOF

echo "âœ… Created terraform.tfvars"

# Show configuration
echo ""
echo "ğŸ“‹ Configuration Summary:"
echo "  Project:      $PROJECT_ID"
echo "  Region:       us-west1"
echo "  GPU Type:     $GPU_TYPE"
echo "  GPUs/Node:    2"
echo "  Machine Type: $MACHINE_TYPE"
echo "  Spot:         Yes (70% discount)"
echo ""

# Initialize Terraform
echo "ğŸ”§ Initializing Terraform..."
terraform init

# Show plan
echo ""
echo "ğŸ“Š Terraform plan:"
terraform plan

# Confirm
echo ""
echo "âš ï¸  COST WARNING:"
if [ "$GPU_CHOICE" == "2" ]; then
    echo "  - ~\$1.40/hr per GPU node (2x L4 spot)"
    echo "  - ~\$3 for a 2-hour test session"
else
    echo "  - ~\$0.70/hr per GPU node (2x T4 spot)"
    echo "  - ~\$1.50 for a 2-hour test session"
fi
echo "  - Cluster auto-scales to 0 when idle (saves money)"
echo "  - REMEMBER: Run 'terraform destroy' when done!"
echo ""
read -p "Create multi-GPU cluster? (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Cancelled."
    exit 0
fi

# Apply
echo ""
echo "ğŸ—ï¸  Creating multi-GPU cluster (this takes ~10-15 minutes)..."
terraform apply -auto-approve

# Get credentials
echo ""
echo "ğŸ”‘ Configuring kubectl..."
eval $(terraform output -raw connect_command)

# Verify
echo ""
echo "âœ… Multi-GPU cluster created successfully!"
echo ""
echo "ğŸ“Š Cluster nodes:"
kubectl get nodes -o wide
echo ""
echo "ğŸ® GPU allocation (may show 0 if auto-scaled down):"
kubectl get nodes -o custom-columns=NAME:.metadata.name,GPU:.status.allocatable."nvidia\.com/gpu"
echo ""

# Test GPU allocation
echo "ğŸ§ª Testing 2-GPU allocation..."
cat > /tmp/multi-gpu-test.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: multi-gpu-test
spec:
  restartPolicy: OnFailure
  containers:
  - name: cuda-test
    image: nvidia/cuda:12.2.0-base-ubuntu22.04
    command: ["nvidia-smi"]
    resources:
      limits:
        nvidia.com/gpu: 2
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
  nodeSelector:
    cloud.google.com/gke-nodepool: gpu-pool
EOF

kubectl apply -f /tmp/multi-gpu-test.yaml
echo "â³ Waiting for GPU test pod (may take 2-3 min if scaling up node)..."
kubectl wait --for=condition=Ready pod/multi-gpu-test --timeout=300s || echo "Pod still pending (node may be starting)..."

if kubectl get pod multi-gpu-test -o jsonpath='{.status.phase}' | grep -q "Running\|Succeeded"; then
    echo ""
    echo "âœ… GPU Test Results:"
    kubectl logs multi-gpu-test
    echo ""
    echo "âœ… SUCCESS: 2 GPUs detected!"
else
    echo ""
    echo "â³ Pod still pending. Check status with:"
    echo "   kubectl get pod multi-gpu-test"
    echo "   kubectl describe pod multi-gpu-test"
fi

kubectl delete pod multi-gpu-test 2>/dev/null || true
rm /tmp/multi-gpu-test.yaml

# Next steps
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… Multi-GPU Cluster Ready!"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ğŸ“š Next Steps:"
echo ""
echo "1ï¸âƒ£  Build and deploy multi-GPU controller:"
echo "   cd ../../"
echo "   export IMG=gcr.io/$PROJECT_ID/llmkube-controller:v0.3.0-multi-gpu"
echo "   make docker-build IMG=\$IMG"
echo "   make docker-push IMG=\$IMG"
echo "   make install"
echo "   make deploy IMG=\$IMG"
echo ""
echo "2ï¸âƒ£  Deploy multi-GPU test model (13B):"
echo "   kubectl apply -f config/samples/multi-gpu-llama-13b-model.yaml"
echo ""
echo "3ï¸âƒ£  Follow the test plan:"
echo "   See: test/e2e/multi-gpu-test-plan.md"
echo "   Or: MULTI-GPU-DEPLOYMENT.md for full guide"
echo ""
echo "ğŸ’° IMPORTANT: When done testing:"
echo "   terraform destroy"
echo ""
echo "ğŸ“Š Monitor costs:"
echo "   https://console.cloud.google.com/billing"
echo ""
