#!/bin/bash
# Multi-GPU Quick Start Script for EKS
# For testing Issue #2 - Multi-GPU single-node support

set -e

echo "ğŸš€ LLMKube Multi-GPU EKS Cluster Setup"
echo "======================================"
echo "This will create a cluster with 4 GPUs per node for multi-GPU testing"
echo ""

# Check if AWS CLI is installed
if ! command -v aws &> /dev/null; then
    echo "âŒ AWS CLI not found. Installing..."
    if [[ "$OSTYPE" == "darwin"* ]]; then
        brew install awscli
    else
        echo "Please install AWS CLI: https://aws.amazon.com/cli/"
        exit 1
    fi
fi

# Check if terraform is installed
if ! command -v terraform &> /dev/null; then
    echo "âŒ Terraform not found. Installing..."
    if [[ "$OSTYPE" == "darwin"* ]]; then
        brew install terraform
    else
        echo "Please install Terraform: https://www.terraform.io/downloads"
        exit 1
    fi
fi

# Check if kubectl is installed
if ! command -v kubectl &> /dev/null; then
    echo "âŒ kubectl not found. Installing..."
    if [[ "$OSTYPE" == "darwin"* ]]; then
        brew install kubectl
    else
        echo "Please install kubectl: https://kubernetes.io/docs/tasks/tools/"
        exit 1
    fi
fi

# Check if AWS credentials are configured
if ! aws sts get-caller-identity &> /dev/null; then
    echo "ğŸ” AWS credentials not configured. Running aws configure..."
    aws configure
fi

# Get AWS account info
AWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text)
AWS_REGION=$(aws configure get region || echo "us-east-1")

echo "âœ… Using AWS Account: $AWS_ACCOUNT"
echo "âœ… Using Region: $AWS_REGION"
echo ""

# Ask user to select GPU configuration
echo "ğŸ® Select GPU configuration:"
echo "  1) 4x T4 GPUs (Recommended - Cost-effective ~$1.15/hr per node)"
echo "  2) 4x A10G GPUs (Better performance ~$1.63/hr per node)"
read -p "Choice (1 or 2): " GPU_CHOICE

if [ "$GPU_CHOICE" == "2" ]; then
    GPU_INSTANCE="g5.12xlarge"
    GPU_TYPE="nvidia-a10g"
    GPU_COUNT=4
    GPU_NAME="A10G"
    COST_HR="1.63"
else
    GPU_INSTANCE="g4dn.12xlarge"
    GPU_TYPE="tesla-t4"
    GPU_COUNT=4
    GPU_NAME="T4"
    COST_HR="1.15"
fi

echo "âœ… Selected: ${GPU_COUNT}x $GPU_NAME GPUs on $GPU_INSTANCE"

# Create terraform.tfvars
echo ""
echo "ğŸ“ Creating terraform.tfvars for multi-GPU configuration..."
cat > terraform.tfvars <<EOF
# Multi-GPU Configuration for LLMKube Testing (Issue #2)
# Auto-generated by multi-gpu-quick-start.sh

region       = "$AWS_REGION"
cluster_name = "llmkube-multi-gpu-test"

# Multi-GPU Configuration: $GPU_COUNT GPUs per node
gpu_instance_type = "$GPU_INSTANCE"
gpu_type          = "$GPU_TYPE"
gpu_count         = $GPU_COUNT

# Auto-scaling (start at 0 to save money)
min_gpu_nodes = 0
max_gpu_nodes = 2

# Cost savings (~70% discount)
use_spot = true

# Storage (larger for 13B+ models)
disk_size_gb = 200
EOF

echo "âœ… Created terraform.tfvars"

# Show configuration
echo ""
echo "ğŸ“‹ Configuration Summary:"
echo "  AWS Account:  $AWS_ACCOUNT"
echo "  Region:       $AWS_REGION"
echo "  GPU Instance: $GPU_INSTANCE"
echo "  GPU Type:     $GPU_TYPE"
echo "  GPUs/Node:    $GPU_COUNT"
echo "  Spot:         Yes (70% discount)"
echo ""

# Initialize Terraform
echo "ğŸ”§ Initializing Terraform..."
terraform init

# Show plan
echo ""
echo "ğŸ“Š Terraform plan:"
terraform plan

# Confirm
echo ""
echo "âš ï¸  COST WARNING:"
echo "  - ~\$${COST_HR}/hr per GPU node (${GPU_COUNT}x $GPU_NAME spot)"
echo "  - ~\$$(echo "$COST_HR * 2" | bc) for a 2-hour test session"
echo "  - Cluster auto-scales to 0 when idle (saves money)"
echo "  - REMEMBER: Run 'terraform destroy' when done!"
echo ""
read -p "Create multi-GPU cluster? (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Cancelled."
    exit 0
fi

# Apply
echo ""
echo "ğŸ—ï¸  Creating multi-GPU cluster (this takes ~15-20 minutes)..."
terraform apply -auto-approve

# Get credentials
echo ""
echo "ğŸ”‘ Configuring kubectl..."
eval $(terraform output -raw connect_command)

# Verify
echo ""
echo "âœ… Multi-GPU cluster created successfully!"
echo ""
echo "ğŸ“Š Cluster nodes:"
kubectl get nodes -o wide
echo ""
echo "ğŸ® GPU allocation:"
kubectl get nodes -o custom-columns=NAME:.metadata.name,GPU:.status.allocatable.\"nvidia\\.com/gpu\"
echo ""

# Test GPU allocation
echo "ğŸ§ª Testing ${GPU_COUNT}-GPU allocation..."
cat > /tmp/multi-gpu-test.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: multi-gpu-test
spec:
  restartPolicy: OnFailure
  containers:
  - name: cuda-test
    image: nvidia/cuda:12.2.0-base-ubuntu22.04
    command: ["nvidia-smi"]
    resources:
      limits:
        nvidia.com/gpu: $GPU_COUNT
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
EOF

kubectl apply -f /tmp/multi-gpu-test.yaml
echo "â³ Waiting for GPU test pod (may take 2-3 min if scaling up node)..."
kubectl wait --for=condition=Ready pod/multi-gpu-test --timeout=300s || echo "Pod still pending (node may be starting)..."

if kubectl get pod multi-gpu-test -o jsonpath='{.status.phase}' | grep -q "Running\\|Succeeded"; then
    echo ""
    echo "âœ… GPU Test Results:"
    kubectl logs multi-gpu-test
    echo ""
    echo "âœ… SUCCESS: $GPU_COUNT GPUs detected!"
else
    echo ""
    echo "â³ Pod still pending. Check status with:"
    echo "   kubectl get pod multi-gpu-test"
    echo "   kubectl describe pod multi-gpu-test"
fi

kubectl delete pod multi-gpu-test 2>/dev/null || true
rm /tmp/multi-gpu-test.yaml

# Next steps
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… Multi-GPU Cluster Ready!"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ğŸ“š Next Steps:"
echo ""
echo "1ï¸âƒ£  Build and deploy multi-GPU controller:"
echo "   cd ../../"
echo "   export AWS_ACCOUNT_ID=\$(aws sts get-caller-identity --query Account --output text)"
echo "   export AWS_REGION=$AWS_REGION"
echo "   export IMG=\${AWS_ACCOUNT_ID}.dkr.ecr.\${AWS_REGION}.amazonaws.com/llmkube-controller:v0.3.0-multi-gpu"
echo ""
echo "   # Create ECR repository"
echo "   aws ecr create-repository --repository-name llmkube-controller --region \$AWS_REGION || true"
echo "   aws ecr get-login-password --region \$AWS_REGION | docker login --username AWS --password-stdin \${AWS_ACCOUNT_ID}.dkr.ecr.\${AWS_REGION}.amazonaws.com"
echo ""
echo "   # Build and push"
echo "   make docker-build IMG=\$IMG"
echo "   make docker-push IMG=\$IMG"
echo "   make install"
echo "   make deploy IMG=\$IMG"
echo ""
echo "2ï¸âƒ£  Deploy multi-GPU test model (13B):"
echo "   kubectl apply -f config/samples/multi-gpu-llama-13b-model.yaml"
echo ""
echo "3ï¸âƒ£  Follow the test plan:"
echo "   See: test/e2e/multi-gpu-test-plan.md"
echo "   Or: docs/MULTI-GPU-DEPLOYMENT.md for full guide"
echo ""
echo "ğŸ’° IMPORTANT: When done testing:"
echo "   terraform destroy"
echo ""
echo "ğŸ“Š Monitor costs:"
echo "   https://console.aws.amazon.com/cost-management/home"
echo ""
