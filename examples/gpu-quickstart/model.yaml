apiVersion: inference.llmkube.dev/v1alpha1
kind: Model
metadata:
  name: llama-3b-gpu
  namespace: default
spec:
  source: https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q8_0.gguf
  format: gguf
  quantization: Q8_0
  hardware:
    accelerator: cuda
    gpu:
      enabled: true
      count: 1
      vendor: nvidia
      layers: -1  # -1 means all layers (automatic)
      memory: "8Gi"
  resources:
    cpu: "2"
    memory: "4Gi"
