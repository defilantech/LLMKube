# LLMKube Model Catalog
# Version: v1.1
# Last Updated: 2026-02-07
#
# This catalog contains pre-configured, battle-tested LLM models optimized
# for various use cases. Each model includes verified GGUF sources and
# optimal deployment settings based on benchmarking.

version: "1.1"
models:
  # ============================================================================
  # Small Models (1-3B) - Fast & Efficient
  # ============================================================================

  llama-3.2-3b:
    name: "Llama 3.2 3B Instruct"
    description: "Meta's latest small model. Good quality for size, mobile-friendly."
    size: "3B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf"
    context_size: 8192
    gpu_layers: 26
    use_cases:
      - "general-chat"
      - "lightweight-assistant"
    resources:
      cpu: "2"
      memory: "4Gi"
      gpu_memory: "4Gi"
    vram_estimate: "2-4GB"
    tags:
      - "meta"
      - "llama"
      - "small"
      - "efficient"
    homepage: "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"

  phi-4-mini:
    name: "Phi-4 Mini Instruct (3.8B)"
    description: "Microsoft's latest small model with strong reasoning and math capabilities."
    size: "3.8B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/microsoft_Phi-4-mini-instruct-GGUF/resolve/main/microsoft_Phi-4-mini-instruct-Q5_K_M.gguf"
    context_size: 128000
    gpu_layers: 32
    use_cases:
      - "reasoning"
      - "compact-deployment"
      - "long-context"
    resources:
      cpu: "2"
      memory: "4Gi"
      gpu_memory: "4Gi"
    vram_estimate: "2-4GB"
    tags:
      - "microsoft"
      - "phi"
      - "small"
      - "reasoning"
      - "long-context"
    homepage: "https://huggingface.co/microsoft/Phi-4-mini-instruct"

  gemma-3-4b:
    name: "Gemma 3 4B IT"
    description: "Google's small efficient model. Great quality-to-size ratio for edge deployments."
    size: "4B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/resolve/main/google_gemma-3-4b-it-Q5_K_M.gguf"
    context_size: 8192
    gpu_layers: 26
    use_cases:
      - "general-chat"
      - "lightweight-assistant"
      - "edge-deployment"
    resources:
      cpu: "2"
      memory: "4Gi"
      gpu_memory: "4Gi"
    vram_estimate: "3-4GB"
    tags:
      - "google"
      - "gemma"
      - "small"
      - "efficient"
    homepage: "https://huggingface.co/google/gemma-3-4b-it"

  # ============================================================================
  # Medium Models (7-8B) - Sweet Spot
  # ============================================================================

  llama-3.1-8b:
    name: "Llama 3.1 8B Instruct"
    description: "Most popular open model (5M+ downloads). Reliable, proven workhorse."
    size: "8B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf"
    context_size: 131072
    gpu_layers: 33
    use_cases:
      - "general-purpose"
      - "chat"
      - "production"
    resources:
      cpu: "4"
      memory: "8Gi"
      gpu_memory: "8Gi"
    vram_estimate: "5-8GB"
    tags:
      - "meta"
      - "llama"
      - "popular"
      - "recommended"
      - "long-context"
    homepage: "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct"

  mistral-7b:
    name: "Mistral 7B Instruct v0.3"
    description: "Fast inference, concise responses. Battle-tested since 2023."
    size: "7B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q5_K_M.gguf"
    context_size: 32768
    gpu_layers: 32
    use_cases:
      - "chat"
      - "general-assistant"
      - "fast-inference"
    resources:
      cpu: "4"
      memory: "8Gi"
      gpu_memory: "8Gi"
    vram_estimate: "5-8GB"
    tags:
      - "mistral"
      - "fast"
      - "battle-tested"
    homepage: "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3"

  qwen-2.5-coder-7b:
    name: "Qwen 2.5 Coder 7B Instruct"
    description: "Best-in-class for code generation. Supports 100+ programming languages."
    size: "7B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q5_K_M.gguf"
    context_size: 131072
    gpu_layers: 33
    use_cases:
      - "code-generation"
      - "debugging"
      - "technical-docs"
    resources:
      cpu: "4"
      memory: "8Gi"
      gpu_memory: "8Gi"
    vram_estimate: "5-8GB"
    tags:
      - "qwen"
      - "code"
      - "multilingual"
      - "long-context"
      - "recommended"
    homepage: "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct"

  deepseek-r1-7b:
    name: "DeepSeek R1 Distill Qwen 7B"
    description: "DeepSeek's reasoning model distilled into 7B. Strong chain-of-thought and math capabilities."
    size: "7B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q5_K_M.gguf"
    context_size: 32768
    gpu_layers: 32
    use_cases:
      - "reasoning"
      - "math-reasoning"
      - "problem-solving"
    resources:
      cpu: "4"
      memory: "8Gi"
      gpu_memory: "8Gi"
    vram_estimate: "5-8GB"
    tags:
      - "deepseek"
      - "reasoning"
      - "math"
      - "chain-of-thought"
    homepage: "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"

  gemma-3-12b:
    name: "Gemma 3 12B IT"
    description: "Google's latest Gemma 3 with improved reasoning and multilingual performance."
    size: "12B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/google_gemma-3-12b-it-GGUF/resolve/main/google_gemma-3-12b-it-Q5_K_M.gguf"
    context_size: 8192
    gpu_layers: 48
    use_cases:
      - "general-purpose"
      - "multilingual"
      - "chat"
    resources:
      cpu: "4"
      memory: "12Gi"
      gpu_memory: "12Gi"
    vram_estimate: "8-12GB"
    tags:
      - "google"
      - "gemma"
      - "multilingual"
    homepage: "https://huggingface.co/google/gemma-3-12b-it"

  # ============================================================================
  # Large Models (13B+) - Premium Quality
  # ============================================================================

  deepseek-r1-14b:
    name: "DeepSeek R1 Distill Qwen 14B"
    description: "DeepSeek's reasoning model distilled into 14B. Excellent chain-of-thought for complex tasks."
    size: "14B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q5_K_M.gguf"
    context_size: 32768
    gpu_layers: 48
    use_cases:
      - "reasoning"
      - "math-reasoning"
      - "complex-tasks"
    resources:
      cpu: "6"
      memory: "16Gi"
      gpu_memory: "16Gi"
    vram_estimate: "10-16GB"
    tags:
      - "deepseek"
      - "reasoning"
      - "math"
      - "chain-of-thought"
    homepage: "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"

  deepseek-r1-32b:
    name: "DeepSeek R1 Distill Qwen 32B"
    description: "DeepSeek's most capable distilled reasoning model. Near-frontier reasoning in a deployable size."
    size: "32B"
    quantization: "Q4_K_M"
    source: "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-32B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf"
    context_size: 32768
    gpu_layers: 64
    use_cases:
      - "complex-reasoning"
      - "math-reasoning"
      - "production"
    resources:
      cpu: "8"
      memory: "24Gi"
      gpu_memory: "24Gi"
    vram_estimate: "18-24GB"
    tags:
      - "deepseek"
      - "reasoning"
      - "math"
      - "chain-of-thought"
      - "32gb-vram"
      - "recommended"
    homepage: "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"

  mistral-small-24b:
    name: "Mistral Small 24B Instruct"
    description: "Mistral's efficient 24B model. Strong general performance with fast inference."
    size: "24B"
    quantization: "Q4_K_M"
    source: "https://huggingface.co/bartowski/Mistral-Small-24B-Instruct-2501-GGUF/resolve/main/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf"
    context_size: 32768
    gpu_layers: 56
    use_cases:
      - "general-purpose"
      - "chat"
      - "production"
    resources:
      cpu: "6"
      memory: "18Gi"
      gpu_memory: "18Gi"
    vram_estimate: "14-18GB"
    tags:
      - "mistral"
      - "efficient"
      - "production"
    homepage: "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"

  qwen-2.5-32b:
    name: "Qwen 2.5 32B Instruct"
    description: "Powerful 32B model with excellent reasoning and multilingual capabilities. Perfect for 32GB VRAM setups."
    size: "32B"
    quantization: "Q4_K_M"
    source: "https://huggingface.co/bartowski/Qwen2.5-32B-Instruct-GGUF/resolve/main/Qwen2.5-32B-Instruct-Q4_K_M.gguf"
    context_size: 8192
    gpu_layers: 64
    use_cases:
      - "complex-reasoning"
      - "multilingual"
      - "production"
      - "high-quality-chat"
    resources:
      cpu: "8"
      memory: "24Gi"
      gpu_memory: "24Gi"
    vram_estimate: "18-24GB"
    tags:
      - "qwen"
      - "large"
      - "recommended"
      - "32gb-vram"
      - "long-context"
    homepage: "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"

  qwen-2.5-coder-32b:
    name: "Qwen 2.5 Coder 32B Instruct"
    description: "State-of-the-art coding model matching GPT-4o. Best open-source code LLM available."
    size: "32B"
    quantization: "Q4_K_M"
    source: "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf"
    context_size: 8192
    gpu_layers: 64
    use_cases:
      - "code-generation"
      - "debugging"
      - "code-review"
      - "technical-docs"
    resources:
      cpu: "8"
      memory: "24Gi"
      gpu_memory: "24Gi"
    vram_estimate: "18-24GB"
    tags:
      - "qwen"
      - "code"
      - "large"
      - "recommended"
      - "32gb-vram"
      - "long-context"
      - "gpt4-level"
    homepage: "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct"

  qwen-3-32b:
    name: "Qwen 3 32B"
    description: "Latest Qwen 3 series with hybrid thinking modes. Newest and most capable 32B model."
    size: "32B"
    quantization: "Q4_K_M"
    source: "https://huggingface.co/Qwen/Qwen3-32B-GGUF/resolve/main/Qwen3-32B-Q4_K_M.gguf"
    context_size: 8192
    gpu_layers: 64
    use_cases:
      - "complex-reasoning"
      - "thinking-mode"
      - "production"
      - "high-quality-chat"
    resources:
      cpu: "8"
      memory: "24Gi"
      gpu_memory: "24Gi"
    vram_estimate: "18-24GB"
    tags:
      - "qwen"
      - "large"
      - "newest"
      - "32gb-vram"
      - "long-context"
      - "thinking"
    homepage: "https://huggingface.co/Qwen/Qwen3-32B"

  qwen-2.5-14b:
    name: "Qwen 2.5 14B Instruct"
    description: "Multilingual powerhouse with strong creative writing capabilities."
    size: "14B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q5_K_M.gguf"
    context_size: 131072
    gpu_layers: 48
    use_cases:
      - "multilingual"
      - "creative-writing"
      - "complex-tasks"
    resources:
      cpu: "6"
      memory: "16Gi"
      gpu_memory: "16Gi"
    vram_estimate: "10-16GB"
    tags:
      - "qwen"
      - "multilingual"
      - "creative"
      - "long-context"
    homepage: "https://huggingface.co/Qwen/Qwen2.5-14B-Instruct"

  mixtral-8x7b:
    name: "Mixtral 8x7B Instruct"
    description: "Mixture of Experts (MoE) architecture. 46B effective parameters."
    size: "8x7B"
    quantization: "Q4_K_M"
    source: "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf"
    context_size: 32768
    gpu_layers: 32
    use_cases:
      - "high-quality-chat"
      - "general-purpose"
      - "production"
    resources:
      cpu: "8"
      memory: "32Gi"
      gpu_memory: "32Gi"
    vram_estimate: "24-32GB"
    tags:
      - "mistral"
      - "moe"
      - "large"
      - "high-quality"
    homepage: "https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"

  llama-3.3-70b:
    name: "Llama 3.3 70B Instruct"
    description: "Meta's latest 70B. Improved quality over 3.1 for production workloads."
    size: "70B"
    quantization: "Q4_K_M"
    source: "https://huggingface.co/bartowski/Llama-3.3-70B-Instruct-GGUF/resolve/main/Llama-3.3-70B-Instruct-Q4_K_M.gguf"
    context_size: 131072
    gpu_layers: 80
    use_cases:
      - "flagship-performance"
      - "production"
      - "complex-reasoning"
    resources:
      cpu: "8"
      memory: "80Gi"
      gpu_memory: "80Gi"
    vram_estimate: "40-80GB"
    tags:
      - "meta"
      - "llama"
      - "flagship"
      - "large"
      - "multi-gpu"
      - "long-context"
    homepage: "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"
    notes: "Requires multi-GPU setup (2x A100 40GB or 1x A100 80GB)"
