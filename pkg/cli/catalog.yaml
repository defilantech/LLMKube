# LLMKube Model Catalog
# Version: v1
# Last Updated: 2025-12-01
#
# This catalog contains pre-configured, battle-tested LLM models optimized
# for various use cases. Each model includes verified GGUF sources and
# optimal deployment settings based on benchmarking.

version: "1.0"
models:
  # ============================================================================
  # Small Models (1-3B) - Fast & Efficient
  # ============================================================================

  llama-3.2-3b:
    name: "Llama 3.2 3B Instruct"
    description: "Meta's latest small model. Good quality for size, mobile-friendly."
    size: "3B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf"
    context_size: 8192
    gpu_layers: 26
    use_cases:
      - "general-chat"
      - "lightweight-assistant"
    resources:
      cpu: "2"
      memory: "4Gi"
      gpu_memory: "4Gi"
    vram_estimate: "2-4GB"
    tags:
      - "meta"
      - "llama"
      - "small"
      - "efficient"
    homepage: "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"

  phi-3-mini:
    name: "Phi-3 Mini (3.8B)"
    description: "Microsoft's efficient model with exceptional reasoning for its size."
    size: "3.8B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/Phi-3-mini-128k-instruct-GGUF/resolve/main/Phi-3-mini-128k-instruct-Q5_K_M.gguf"
    context_size: 128000
    gpu_layers: 32
    use_cases:
      - "reasoning"
      - "compact-deployment"
      - "long-context"
    resources:
      cpu: "2"
      memory: "4Gi"
      gpu_memory: "4Gi"
    vram_estimate: "2-4GB"
    tags:
      - "microsoft"
      - "phi"
      - "small"
      - "reasoning"
      - "long-context"
    homepage: "https://huggingface.co/microsoft/Phi-3-mini-128k-instruct"

  # ============================================================================
  # Medium Models (7-8B) - Sweet Spot
  # ============================================================================

  llama-3.1-8b:
    name: "Llama 3.1 8B Instruct"
    description: "Most popular open model (5M+ downloads). Reliable, proven workhorse."
    size: "8B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf"
    context_size: 131072
    gpu_layers: 33
    use_cases:
      - "general-purpose"
      - "chat"
      - "production"
    resources:
      cpu: "4"
      memory: "8Gi"
      gpu_memory: "8Gi"
    vram_estimate: "5-8GB"
    tags:
      - "meta"
      - "llama"
      - "popular"
      - "recommended"
      - "long-context"
    homepage: "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct"

  mistral-7b:
    name: "Mistral 7B Instruct v0.3"
    description: "Fast inference, concise responses. Battle-tested since 2023."
    size: "7B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q5_K_M.gguf"
    context_size: 32768
    gpu_layers: 32
    use_cases:
      - "chat"
      - "general-assistant"
      - "fast-inference"
    resources:
      cpu: "4"
      memory: "8Gi"
      gpu_memory: "8Gi"
    vram_estimate: "5-8GB"
    tags:
      - "mistral"
      - "fast"
      - "battle-tested"
    homepage: "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3"

  qwen-2.5-coder-7b:
    name: "Qwen 2.5 Coder 7B Instruct"
    description: "Best-in-class for code generation. Supports 100+ programming languages."
    size: "7B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q5_K_M.gguf"
    context_size: 131072
    gpu_layers: 33
    use_cases:
      - "code-generation"
      - "debugging"
      - "technical-docs"
    resources:
      cpu: "4"
      memory: "8Gi"
      gpu_memory: "8Gi"
    vram_estimate: "5-8GB"
    tags:
      - "qwen"
      - "code"
      - "multilingual"
      - "long-context"
      - "recommended"
    homepage: "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct"

  deepseek-coder-6.7b:
    name: "DeepSeek Coder 6.7B"
    description: "Exceptional code comprehension and mathematical reasoning."
    size: "6.7B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/deepseek-coder-6.7b-instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct-Q5_K_M.gguf"
    context_size: 16384
    gpu_layers: 32
    use_cases:
      - "code-generation"
      - "math-reasoning"
      - "problem-solving"
    resources:
      cpu: "4"
      memory: "8Gi"
      gpu_memory: "8Gi"
    vram_estimate: "5-8GB"
    tags:
      - "deepseek"
      - "code"
      - "reasoning"
      - "math"
    homepage: "https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct"

  gemma-2-9b:
    name: "Gemma 2 9B Instruct"
    description: "Google's efficient model with strong multilingual performance."
    size: "9B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf"
    context_size: 8192
    gpu_layers: 42
    use_cases:
      - "general-purpose"
      - "multilingual"
      - "chat"
    resources:
      cpu: "4"
      memory: "10Gi"
      gpu_memory: "10Gi"
    vram_estimate: "6-10GB"
    tags:
      - "google"
      - "gemma"
      - "multilingual"
    homepage: "https://huggingface.co/google/gemma-2-9b-it"

  # ============================================================================
  # Large Models (13B+) - Premium Quality
  # ============================================================================

  qwen-2.5-32b:
    name: "Qwen 2.5 32B Instruct"
    description: "Powerful 32B model with excellent reasoning and multilingual capabilities. Perfect for 32GB VRAM setups."
    size: "32B"
    quantization: "Q4_K_M"
    source: "https://huggingface.co/bartowski/Qwen2.5-32B-Instruct-GGUF/resolve/main/Qwen2.5-32B-Instruct-Q4_K_M.gguf"
    context_size: 8192
    gpu_layers: 64
    use_cases:
      - "complex-reasoning"
      - "multilingual"
      - "production"
      - "high-quality-chat"
    resources:
      cpu: "8"
      memory: "24Gi"
      gpu_memory: "24Gi"
    vram_estimate: "18-24GB"
    tags:
      - "qwen"
      - "large"
      - "recommended"
      - "32gb-vram"
      - "long-context"
    homepage: "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"

  qwen-2.5-coder-32b:
    name: "Qwen 2.5 Coder 32B Instruct"
    description: "State-of-the-art coding model matching GPT-4o. Best open-source code LLM available."
    size: "32B"
    quantization: "Q4_K_M"
    source: "https://huggingface.co/bartowski/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf"
    context_size: 8192
    gpu_layers: 64
    use_cases:
      - "code-generation"
      - "debugging"
      - "code-review"
      - "technical-docs"
    resources:
      cpu: "8"
      memory: "24Gi"
      gpu_memory: "24Gi"
    vram_estimate: "18-24GB"
    tags:
      - "qwen"
      - "code"
      - "large"
      - "recommended"
      - "32gb-vram"
      - "long-context"
      - "gpt4-level"
    homepage: "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct"

  qwen-3-32b:
    name: "Qwen 3 32B"
    description: "Latest Qwen 3 series with hybrid thinking modes. Newest and most capable 32B model."
    size: "32B"
    quantization: "Q4_K_M"
    source: "https://huggingface.co/Qwen/Qwen3-32B-GGUF/resolve/main/Qwen3-32B-Q4_K_M.gguf"
    context_size: 8192
    gpu_layers: 64
    use_cases:
      - "complex-reasoning"
      - "thinking-mode"
      - "production"
      - "high-quality-chat"
    resources:
      cpu: "8"
      memory: "24Gi"
      gpu_memory: "24Gi"
    vram_estimate: "18-24GB"
    tags:
      - "qwen"
      - "large"
      - "newest"
      - "32gb-vram"
      - "long-context"
      - "thinking"
    homepage: "https://huggingface.co/Qwen/Qwen3-32B"

  qwen-2.5-14b:
    name: "Qwen 2.5 14B Instruct"
    description: "Multilingual powerhouse with strong creative writing capabilities."
    size: "14B"
    quantization: "Q5_K_M"
    source: "https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q5_K_M.gguf"
    context_size: 131072
    gpu_layers: 48
    use_cases:
      - "multilingual"
      - "creative-writing"
      - "complex-tasks"
    resources:
      cpu: "6"
      memory: "16Gi"
      gpu_memory: "16Gi"
    vram_estimate: "10-16GB"
    tags:
      - "qwen"
      - "multilingual"
      - "creative"
      - "long-context"
    homepage: "https://huggingface.co/Qwen/Qwen2.5-14B-Instruct"

  mixtral-8x7b:
    name: "Mixtral 8x7B Instruct"
    description: "Mixture of Experts (MoE) architecture. 46B effective parameters."
    size: "8x7B"
    quantization: "Q4_K_M"
    source: "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf"
    context_size: 32768
    gpu_layers: 32
    use_cases:
      - "high-quality-chat"
      - "general-purpose"
      - "production"
    resources:
      cpu: "8"
      memory: "32Gi"
      gpu_memory: "32Gi"
    vram_estimate: "24-32GB"
    tags:
      - "mistral"
      - "moe"
      - "large"
      - "high-quality"
    homepage: "https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"

  llama-3.1-70b:
    name: "Llama 3.1 70B Instruct"
    description: "Flagship open model. GPT-3.5+ quality for production workloads."
    size: "70B"
    quantization: "Q4_K_M"
    source: "https://huggingface.co/bartowski/Meta-Llama-3.1-70B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf"
    context_size: 131072
    gpu_layers: 80
    use_cases:
      - "flagship-performance"
      - "production"
      - "complex-reasoning"
    resources:
      cpu: "8"
      memory: "80Gi"
      gpu_memory: "80Gi"
    vram_estimate: "40-80GB"
    tags:
      - "meta"
      - "llama"
      - "flagship"
      - "large"
      - "multi-gpu"
      - "long-context"
    homepage: "https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct"
    notes: "Requires multi-GPU setup (2x A100 40GB or 1x A100 80GB)"
